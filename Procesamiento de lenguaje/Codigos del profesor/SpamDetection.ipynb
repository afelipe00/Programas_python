{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"SpamDetection.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"d4k5Q5fEPpxH","colab_type":"text"},"source":["<h1>Dataset</h1>"]},{"cell_type":"markdown","metadata":{"id":"uVhTmV1VPpxK","colab_type":"text"},"source":["Abra con su editor de texto preferido el dataset smsspamcollection.\n","\n","* Las columnas en el conjunto de datos actualmente no tienen nombre y, como puede ver, hay 2 columnas.\n","\n","* La primera columna toma dos valores, 'ham', que significa que el mensaje no es spam, y 'spam', que significa que el mensaje es spam.\n","\n","* La segunda columna es el contenido de texto del mensaje SMS que se est√° clasificando."]},{"cell_type":"code","metadata":{"id":"mwmz52nXPpxL","colab_type":"code","colab":{},"outputId":"38e8adf4-376c-459a-99ab-3e516e71321c"},"source":["import pandas as pd\n","# Dataset from - https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n","df = pd.read_table('smsspamcollection/SMSSpamCollection',\n","                   sep='\\t', \n","                   header=None, \n","                   names=['label', 'sms_message'])\n","\n","# imprima las primeras 5 filas\n","df.head()\n","\n","#Cambiamos las etiquetas texutales por etiquetas numericas, esto es una buena practica cuando se construyen modelo supervisados\n","df['label'] = df.label.map({'ham':0, 'spam':1})\n","print(df.shape)\n","df.head() # returns (rows, columns)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(5572, 2)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>sms_message</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Go until jurong point, crazy.. Available only ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>Ok lar... Joking wif u oni...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>U dun say so early hor... U c already then say...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>Nah I don't think he goes to usf, he lives aro...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   label                                        sms_message\n","0      0  Go until jurong point, crazy.. Available only ...\n","1      0                      Ok lar... Joking wif u oni...\n","2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n","3      0  U dun say so early hor... U c already then say...\n","4      0  Nah I don't think he goes to usf, he lives aro..."]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"h3qIOrtCPpxR","colab_type":"text"},"source":["<h2>Construir la bolsa de palabras con SKLEARN</h2>"]},{"cell_type":"code","metadata":{"id":"x6_7u112PpxR","colab_type":"code","colab":{}},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","count_vector = CountVectorizer()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PLu8Q7CdPpxU","colab_type":"text"},"source":["** Data preprocessing with CountVectorizer() SKLEARN ** \n","\n","SKLEARN CountVectorizer involved converting all of our data to lower case and removing all punctuation marks. CountVectorizer() has certain parameters which take care of these steps for us. They are:\n","\n","* `lowercase = True`\n","    \n","    The `lowercase` parameter has a default value of `True` which converts all of our text to its lower case form.\n","\n","\n","* `token_pattern = (?u)\\\\b\\\\w\\\\w+\\\\b`\n","    \n","    The `token_pattern` parameter has a default regular expression value of `(?u)\\\\b\\\\w\\\\w+\\\\b` which ignores all punctuation marks and treats them as delimiters, while accepting alphanumeric strings of length greater than or equal to 2, as individual tokens or words.\n","\n","\n","* `stop_words`\n","\n","    The `stop_words` parameter, if set to `english` will remove all words from our document set that match a list of English stop words which is defined in scikit-learn. Considering the size of our dataset and the fact that we are dealing with SMS messages and not larger text sources like e-mail, we will not be setting this parameter value.\n","\n","You can take a look at all the parameter values of your `count_vector` object by simply printing out the object as follows:"]},{"cell_type":"code","metadata":{"id":"pF1f6VxtPpxV","colab_type":"code","colab":{},"outputId":"23232a21-02af-450f-8186-eb5f194cfee5"},"source":["print(count_vector)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n","                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n","                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n","                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n","                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n","                tokenizer=None, vocabulary=None)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"T-syfZSSPpxX","colab_type":"text"},"source":["<h3>Ejemplo de CountVectorizer</h3>"]},{"cell_type":"code","metadata":{"id":"XcrHBpRfPpxY","colab_type":"code","colab":{},"outputId":"20095900-1656-4d76-d000-f7818caa2143"},"source":["documents = ['Hello, how are you!',\n","                'Win money, win from home.',\n","                'Call me now.',\n","                'Hello, Call hello you tomorrow?']\n","\n","count_vector.fit(documents)\n","count_vector.get_feature_names() #Retorna el vocabulario del corpus\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['are',\n"," 'call',\n"," 'from',\n"," 'hello',\n"," 'home',\n"," 'how',\n"," 'me',\n"," 'money',\n"," 'now',\n"," 'tomorrow',\n"," 'win',\n"," 'you']"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"yKI0qA8sPpxb","colab_type":"code","colab":{},"outputId":"b9130231-8969-4837-dcd9-a95d66e15865"},"source":["#Matriz termino documento\n","doc_array = count_vector.transform(documents).toarray()\n","doc_array"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1],\n","       [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 2, 0],\n","       [0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n","       [0, 1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1]], dtype=int64)"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"Shp7TvzSPpxe","colab_type":"code","colab":{},"outputId":"5bb68297-1cd2-4e37-f5a1-949df64281d7"},"source":["#Mas bonito en un data frame de pandas\n","frequency_matrix = pd.DataFrame(doc_array, \n","                                columns = count_vector.get_feature_names())\n","frequency_matrix"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>are</th>\n","      <th>call</th>\n","      <th>from</th>\n","      <th>hello</th>\n","      <th>home</th>\n","      <th>how</th>\n","      <th>me</th>\n","      <th>money</th>\n","      <th>now</th>\n","      <th>tomorrow</th>\n","      <th>win</th>\n","      <th>you</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   are  call  from  hello  home  how  me  money  now  tomorrow  win  you\n","0    1     0     0      1     0    1   0      0    0         0    0    1\n","1    0     0     1      0     1    0   0      1    0         0    2    0\n","2    0     1     0      0     0    0   1      0    1         0    0    0\n","3    0     1     0      2     0    0   0      0    0         1    0    1"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"lYy_r_P0Ppxh","colab_type":"text"},"source":["<h2>Paso 1: Dividir el dataset en Entrenamiento y Pruebas</h2>"]},{"cell_type":"code","metadata":{"id":"HyOIaFEiPpxh","colab_type":"code","colab":{},"outputId":"951b1176-b9db-4895-a119-d67d3d0c433e"},"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(df['sms_message'], \n","                                                    df['label'], \n","                                                    random_state=1)\n","\n","print('Number of rows in the total set: {}'.format(df.shape[0]))\n","print('Number of rows in the training set: {}'.format(X_train.shape[0]))\n","print('Number of rows in the test set: {}'.format(X_test.shape[0]))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of rows in the total set: 5572\n","Number of rows in the training set: 4179\n","Number of rows in the test set: 1393\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"X-4CcF_VPpxj","colab_type":"text"},"source":["<h2>Paso 2: Construir el modelo de bolsa de palabras para nuestro dataset </h2>"]},{"cell_type":"code","metadata":{"id":"IBUgZxI5Ppxl","colab_type":"code","colab":{}},"source":["# Instantiate the CountVectorizer method\n","count_vector = CountVectorizer()\n","\n","# Fit the training data and then return the matrix\n","training_data = count_vector.fit_transform(X_train)\n","\n","# Transform testing data and return the matrix. Note we are not fitting the testing data into the CountVectorizer()\n","testing_data = count_vector.transform(X_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_wcifoLuPpxp","colab_type":"text"},"source":["<h2>Paso 3: Entrenar el clasificador Naive Bayes </h2>"]},{"cell_type":"code","metadata":{"id":"IwjAhTKoPpxp","colab_type":"code","colab":{},"outputId":"f6bb5ced-f049-40fb-c95b-c13810cead86"},"source":["from sklearn.naive_bayes import MultinomialNB\n","# Ver documentacion https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n","naive_bayes = MultinomialNB()\n","naive_bayes.fit(training_data, y_train)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"OYfofdBHPpxs","colab_type":"text"},"source":["<h2>Paso 4: Evaluando el Modelo </h2>"]},{"cell_type":"code","metadata":{"id":"9Z_naMIOPpxs","colab_type":"code","colab":{}},"source":["predictions = naive_bayes.predict(testing_data) #Lo primero es utilizar nuestro modelo para hacer predicciones sobre el dataset de pruebas"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4t-_qmZiPpxv","colab_type":"code","colab":{},"outputId":"c25f65cc-6694-4fb7-e6d5-dc71eb8c9ab5"},"source":["#Ahora vamos a construir la matriz de confusion\n","from sklearn.metrics import confusion_matrix\n","confusion_matrix(y_test, predictions)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1203,    5],\n","       [  11,  174]], dtype=int64)"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"JE9OOiAhPpxx","colab_type":"code","colab":{},"outputId":"13f7fc0f-7b8d-4815-a54e-6e27dba7723e"},"source":["#En terminos de TP, FP, TN, FN\n","tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n","(tn,fp,fn,tp)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1203, 5, 11, 174)"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"yizfzm-2Ppxz","colab_type":"text"},"source":["### Evaluation Metrics ###\n","\n","** Accuracy ** measures how often the classifier makes the correct prediction. It‚Äôs the ratio of the number of correct predictions to the total number of predictions (the number of test data points).\n","\n","** Precision ** tells us what proportion of messages we classified as spam, actually were spam.\n","It is a ratio of true positives(words classified as spam, and which are actually spam) to all positives(all words classified as spam, irrespective of whether that was the correct classification), in other words it is the ratio of\n","\n","`[True Positives/(True Positives + False Positives)]`\n","\n","** Recall(sensitivity)** tells us what proportion of messages that actually were spam were classified by us as spam.\n","It is a ratio of true positives(words classified as spam, and which are actually spam) to all the words that were actually spam, in other words it is the ratio of\n","\n","`[True Positives/(True Positives + False Negatives)]`"]},{"cell_type":"code","metadata":{"id":"QzzZc1s0Ppx0","colab_type":"code","colab":{},"outputId":"84a1cdb9-40b1-46bf-bdee-3c80d9550be3"},"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","print('Accuracy score: ', format(accuracy_score(y_test, predictions)))\n","print('Precision score: ', format(precision_score(y_test, predictions)))\n","print('Recall score: ', format(recall_score(y_test, predictions)))\n","print('F1 score: ', format(f1_score(y_test, predictions)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Accuracy score:  0.9885139985642498\n","Precision score:  0.9720670391061452\n","Recall score:  0.9405405405405406\n","F1 score:  0.9560439560439562\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"y2UAaImzPpx2","colab_type":"text"},"source":["<h2>Implementar su propia version del Clasificador Naive Bayes</h2>"]},{"cell_type":"markdown","metadata":{"id":"ahm0HANfPpx3","colab_type":"text"},"source":["1. Pueden usar scikit-learn para construir la bolsa de palabras y el vocabulario. Implemente su propia versi√≥n del clasificador Naive Bayes, no use la implementaci√≥n de scikit-learn.\n","2. Usen el mismo dataset usado en esta practica.\n","3. Comparen los resultados (P, R, A) obtenidos con y sin scikit-learn. ¬øSon diferentes?¬øA que cree que se debe? (Espero respuestas con contenido)"]},{"cell_type":"code","metadata":{"id":"m_3V9kSQPpx4","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}