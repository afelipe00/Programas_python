{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Documentos</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "documents = [\"Human machine interface for lab abc computer applications\", \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\"System and human system engineering testing of EPS\",\n",
    "             \"Relation of user perceived response time to error measurement\",\"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "             \"Graph minors A survey\", \"Human machine interface for machine learning applications\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Etapa de procesamiento Â¿Que se hace?\n",
    "tok_docs = [] #Documentos procesados\n",
    "vocab = [] #Vocabulario, muy importante!\n",
    "\n",
    "stop_words = set(stopwords.words('english')) #Lista en Ingles\n",
    "for doc in documents:\n",
    "    word_tok = nltk.word_tokenize(doc)\n",
    "    filtered_sentence = [] \n",
    "    for w in word_tok: \n",
    "        w = w.lower()\n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w)\n",
    "            vocab.append(w)\n",
    "    tok_docs.append(filtered_sentence)\n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocabulario y documentos despues de procesamiento\n",
    "print(vocab)\n",
    "print(len(vocab))\n",
    "print(tok_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Esta funcion crea el diccionario TF de cada documento\n",
    "def computeDocsTFDict(doc):\n",
    "    \"\"\" Retorna un diccionario de frecuencias  \n",
    "    con las palabras unicas del documento.\n",
    "    \"\"\"\n",
    "    #Counts the number of times the word appears in review\n",
    "    TFDict = {}\n",
    "    for word in doc:\n",
    "        if word in TFDict:\n",
    "            TFDict[word] += 1\n",
    "        else:\n",
    "            TFDict[word] = 1\n",
    "    #Computes tf for each word           \n",
    "    for word in TFDict:\n",
    "        TFDict[word] = TFDict[word] / len(doc)\n",
    "    return TFDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfDict = [] #En este diccionario se va a almacenar el conteo\n",
    "for doc in tok_docs:\n",
    "    tfDict.append(computeDocsTFDict(doc))\n",
    "tfDict[0] #El indice entre tfDict y tok_docs corresponden a los mismos documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Calculo de IDF</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para el calculo de IDF necesitamos conocer primero cuantas veces una palabra aparace en los documentos del corpus\n",
    "def computeWordCountDict(tfDict):\n",
    "    \"\"\" \n",
    "    Devuelve un diccionario cuyos indices son todas las palabras unicas en el conjunto de datos y cuyos valores cuentan el numero de\n",
    "    documentos en las que aparece la palabra\n",
    "    \"\"\"\n",
    "    countDict = {}\n",
    "\n",
    "    for doc in tfDict:\n",
    "        for word in doc:\n",
    "            if word in countDict:\n",
    "                countDict[word] += 1\n",
    "            else:\n",
    "                countDict[word] = 1\n",
    "    return countDict\n",
    "\n",
    "countDict = computeWordCountDict(tfDict)\n",
    "countDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para el calculo de IDF necesitamos conocer primero cuantas veces una palabra aparace en los documentos del corpus\n",
    "def computeIDFDict(countDict):\n",
    "    \"\"\" Devuelve un diccionario cuyos indices son palabras \n",
    "        y sus valores son el idf correspondiente.\n",
    "    \"\"\"\n",
    "    idfDict = {}\n",
    "    for word in countDict:\n",
    "        idfDict[word] = math.log(len(documents) / countDict[word])\n",
    "    return idfDict\n",
    "  \n",
    "idfDict = computeIDFDict(countDict)\n",
    "idfDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Calculo de TF-IDF</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeDocsTFIDFDict(TFDict,idfDict):\n",
    "    \"\"\" Devuelve un diccionario cuyas claves son todas las palabras unicas en\n",
    "    la revision y cuyos valores son su tfidf correspondiente.\n",
    "    \"\"\"\n",
    "    TFIDFDict = {}\n",
    "    for word in TFDict:\n",
    "        TFIDFDict[word] = TFDict[word] * idfDict[word]\n",
    "    return TFIDFDict\n",
    "\n",
    "tfidfDict = [computeDocsTFIDFDict(doc,idfDict) for doc in tfDict]\n",
    "tfidfDict[0] #El indice entre tfidfDict y tok_docs corresponden a los mismos documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Vectorizacion</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a utilizar el vocabulario alamacenado en vocab como dimensiones\n",
    "def computeTFIDFVector(doc):\n",
    "    tfidfVector = [0.0] * len(vocab) #Vector del tamano del vocabulario\n",
    "    # Para cada palabra unica, si esta en el documento, se almacena su valor TF-IDF.\n",
    "    for i, word in enumerate(vocab):\n",
    "        if word in doc:\n",
    "            tfidfVector[i] = doc[word]\n",
    "    return tfidfVector\n",
    "\n",
    "tfidfVector = [computeTFIDFVector(doc) for doc in tfidfDict]\n",
    "tfidfVector[0] #Vector que representa el primer documento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Similitud Coseno</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(vector_x, vector_y):\n",
    "    dot = 0.0\n",
    "    for e_x, e_y in zip(vector_x, vector_y):\n",
    "       dot += e_x * e_y\n",
    "    return dot\n",
    "\n",
    "def magnitude(vector):\n",
    "    mag = 0.0\n",
    "    for index in vector:\n",
    "        mag += math.pow(index, 2)\n",
    "    return math.sqrt(mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_similarity_0_1 = dot_product(tfidfVector[0], tfidfVector[1])/ magnitude(tfidfVector[0]) * magnitude(tfidfVector[1])\n",
    "doc_similarity_0_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tarea: Buscar los documentos mas similares entre si en nuestro corpus</h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
