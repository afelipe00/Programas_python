{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.porter import PorterStemmer\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "from smart_open import open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runner ran running trail runs\n",
      "the runner ran on the run trail for their run\n"
     ]
    }
   ],
   "source": [
    "#Gensim trae sus propias funciones para el procesamiento de texto\n",
    "text_test = \"the runner ran on the running trail for their runs\"\n",
    "\n",
    "print(remove_stopwords(text_test))\n",
    "p = PorterStemmer()\n",
    "print(p.stem_sentence(text_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Corpus Streaming</h2>\n",
    "Normalmente los corpus reciden completamente en la memoria. Supongamos que hay millones de documentos en el corpus. Almacenarlos todos en RAM no será suficiente. En su lugar, supongamos que los documentos se almacenan en un archivo en el disco, un documento por línea. Gensim puede procesar un documento a la vez y actualizar dinamicamente e calculo de IDF y del vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Procesamiento de un texto utilizando las funciones de Gensim\n",
    "def process(text):\n",
    "    doc_nor= text.lower()\n",
    "    doc_sw = remove_stopwords(doc_nor)\n",
    "    doc_stem = p.stem_sentence(doc_sw)\n",
    "    return doc_stem.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amazon', 'distribut', 'download', 'stream', 'video,', 'music,', 'audiobook', 'amazon', 'prime', 'video,', 'amazon', 'music,', 'audibl', 'subsidiaries.', 'amazon', 'publish', 'arm,', 'amazon', 'publishing,', 'film', 'televis', 'studio,', 'amazon', 'studios,', 'cloud', 'comput', 'subsidiary,', 'amazon', 'web', 'services.']\n"
     ]
    }
   ],
   "source": [
    "#Creaccion de lista con documentos procesados\n",
    "\n",
    "docDict = []\n",
    "for line in open('DocsParaDiccionario.txt', 'rb'):\n",
    "    docDict.append(process(line))\n",
    "\n",
    "print(docDict[0]) #Documentos ya procesados, de esta forma deben ser enviados al metodo de Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(309 unique tokens: ['amazon', 'arm,', 'audibl', 'audiobook', 'cloud']...)\n",
      "{'amazon': 0, 'arm,': 1, 'audibl': 2, 'audiobook': 3, 'cloud': 4, 'comput': 5, 'distribut': 6, 'download': 7, 'film': 8, 'music,': 9, 'prime': 10, 'publish': 11, 'publishing,': 12, 'services.': 13, 'stream': 14, 'studio,': 15, 'studios,': 16, 'subsidiaries.': 17, 'subsidiary,': 18, 'televis': 19, 'video,': 20, 'web': 21, 'american': 22, 'best': 23, 'browsers.': 24, 'compani': 25, 'computers,': 26, 'consum': 27, 'corpor': 28, 'develops,': 29, 'edg': 30, 'electronics,': 31, 'explor': 32, 'headquart': 33, 'internet': 34, 'known': 35, 'licenses,': 36, 'line': 37, 'manufactures,': 38, 'microsoft': 39, 'multin': 40, 'offic': 41, 'oper': 42, 'person': 43, 'product': 44, 'redmond,': 45, 'relat': 46, 'sell': 47, 'softwar': 48, 'software,': 49, 'suite,': 50, 'support': 51, 'systems,': 52, 'technolog': 53, 'washington.': 54, 'window': 55, '14': 56, '1998': 57, '1998.': 58, '4,': 59, '56': 60, 'brin': 61, 'california.': 62, 'control': 63, 'found': 64, 'googl': 65, 'held': 66, 'incorpor': 67, 'larri': 68, 'page': 69, 'percent': 70, 'ph.d.': 71, 'power': 72, 'privat': 73, 'septemb': 74, 'sergei': 75, 'share': 76, 'stanford': 77, 'stock.': 78, 'stockhold': 79, 'student': 80, 'supervot': 81, 'univers': 82, 'vote': 83, '\"train': 84, '(ml)': 85, 'algorithm': 86, 'artifici': 87, 'base': 88, 'build': 89, 'data\",': 90, 'data,': 91, 'decis': 92, 'effect': 93, 'explicit': 94, 'explicitli': 95, 'infer': 96, 'instead.': 97, 'instructions,': 98, 'intelligence.': 99, 'learn': 100, 'machin': 101, 'mathemat': 102, 'model': 103, 'order': 104, 'pattern': 105, 'perform': 106, 'predict': 107, 'program': 108, 'reli': 109, 'sampl': 110, 'scientif': 111, 'seen': 112, 'specif': 113, 'statist': 114, 'studi': 115, 'subset': 116, 'system': 117, 'task': 118, 'task.': 119, 'us': 120, '\"artifici': 121, '\"cognitive\"': 122, '\"learning\"': 123, '\"problem': 124, '(ai),': 125, '(or': 126, 'associ': 127, 'call': 128, 'colloquially,': 129, 'computers)': 130, 'contrast': 131, 'demonstr': 132, 'displai': 133, 'function': 134, 'human': 135, 'humans.': 136, 'intellig': 137, 'intelligence\"': 138, 'intelligence,': 139, 'machines,': 140, 'mimic': 141, 'mind,': 142, 'natur': 143, 'science,': 144, 'solving\".': 145, 'term': 146, 'collect': 147, 'corpus.': 148, 'document': 149, 'frequency,': 150, 'frequency–invers': 151, 'import': 152, 'inform': 153, 'intend': 154, 'numer': 155, 'reflect': 156, 'retrieval,': 157, 'short': 158, 'tfidf,': 159, 'tf–idf': 160, 'word': 161, '(ir)': 162, 'activ': 163, 'content-bas': 164, 'databas': 165, 'describ': 166, 'document,': 167, 'full-text': 168, 'imag': 169, 'indexing.': 170, 'metadata': 171, 'need': 172, 'obtain': 173, 'relev': 174, 'resourc': 175, 'resources.': 176, 'retriev': 177, 'scienc': 178, 'search': 179, 'sounds.': 180, 'texts,': 181, 'themselves,': 182, '(internet': 183, '(serps).': 184, 'carri': 185, 'design': 186, 'engin': 187, 'gener': 188, 'mean': 189, 'particular': 190, 'present': 191, 'query.': 192, 'refer': 193, 'result': 194, 'results,': 195, 'search),': 196, 'specifi': 197, 'systemat': 198, 'textual': 199, 'wai': 200, 'wide': 201, 'world': 202, '(urls,': 203, '(www),': 204, 'access': 205, 'commonli': 206, 'https://www.example.com/),': 207, 'hypertext,': 208, 'identifi': 209, 'interlink': 210, 'internet.': 211, 'locat': 212, 'uniform': 213, 'web,': 214, '(commonli': 215, '(url),': 216, 'applic': 217, 'browser': 218, 'browser)': 219, 'device.': 220, 'distinct': 221, 'enabl': 222, 'image,': 223, 'individu': 224, 'page,': 225, 'server': 226, \"user'\": 227, 'video': 228, 'web.': 229, '(also': 230, '[1]links,': 231, 'act': 232, 'brows': 233, 'contain': 234, 'graphic': 235, 'hyperlinks,': 236, 'hypertext': 237, 'mobil': 238, 'monitor': 239, 'needed.': 240, 'pages.': 241, 'suitabl': 242, 'type': 243, 'typic': 244, 'webpage)': 245, 'written': 246, '(gps)': 247, '(nfc).': 248, '2gor': 249, '3g': 250, '9.0interfac': 251, 'abil': 252, 'allow': 253, 'android': 254, 'app': 255, 'battery.': 256, 'bluetooth,': 257, 'button': 258, 'calls,': 259, 'cameras,': 260, 'capabl': 261, 'car': 262, 'cellular': 263, 'common.': 264, 'commun': 265, 'computer)': 266, 'connect': 267, 'devic': 268, 'digit': 269, 'entertain': 270, 'fhd': 271, 'field': 272, 'flatscreen': 273, 'games,': 274, 'global': 275, 'hand.': 276, 'handheld': 277, 'headset': 278, 'hold': 279, 'instal': 280, 'integr': 281, 'interconnect': 282, 'interface,': 283, 'keyboard': 284, 'keyboard.': 285, 'lcd': 286, 'lithium': 287, 'media': 288, 'network': 289, 'ol': 290, 'or4gnear': 291, 'physic': 292, 'place': 293, 'players,': 294, 'posit': 295, 'provid': 296, 'receiv': 297, 'run': 298, 'run.': 299, 'said': 300, 'small': 301, 'special': 302, 'telephon': 303, 'third-parti': 304, 'touchscreen': 305, 'typically,': 306, 'version': 307, 'wi-fi,': 308}\n"
     ]
    }
   ],
   "source": [
    "#Ahora si llamamos al metodo de Gensim para crear el diccionario a partir de los documentos\n",
    "dictionary = corpora.Dictionary(docDict)\n",
    "dictionary.save('clase3.dict')\n",
    "print(dictionary)\n",
    "\n",
    "#Notese que Gensim de una vez asigna a cada token un id\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(135, 1), (288, 2)]\n"
     ]
    }
   ],
   "source": [
    "new_doc = \"Human media interaction media\"\n",
    "new_doc_bow = dictionary.doc2bow(process(new_doc))\n",
    "print(new_doc_bow)  # ¿Si hay cuatro palabras pq solo dos tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construccion del corpus == Cada documento representado con el vocabulario/diccionario definido anteriormente (Bolsa de palabras)\n",
    "class MyCorpus(object):\n",
    "    def __iter__(self):\n",
    "        for line in open('mycorpus.txt', 'rb'):\n",
    "            yield dictionary.doc2bow(process(line))\n",
    "\n",
    "corpus_memory_friendly = MyCorpus()#Todos mis documentos ahora estan representados como una bolsa de palabras\n",
    "#Almaceno mi corpus\n",
    "corpora.MmCorpus.serialize('corpus.mm', corpus_memory_friendly)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(12 documents, 309 features, 375 non-zero entries)\n",
      "[(0, 7.0), (1, 1.0), (2, 1.0), (3, 1.0), (4, 1.0), (5, 1.0), (6, 1.0), (7, 1.0), (8, 1.0), (9, 2.0), (10, 1.0), (11, 1.0), (12, 1.0), (13, 1.0), (14, 1.0), (15, 1.0), (16, 1.0), (17, 1.0), (18, 1.0), (19, 1.0), (20, 2.0), (21, 1.0)]\n",
      "[(13, 1.0), (21, 1.0), (22, 1.0), (23, 1.0), (24, 1.0), (25, 1.0), (26, 1.0), (27, 1.0), (28, 1.0), (29, 1.0), (30, 1.0), (31, 1.0), (32, 1.0), (33, 1.0), (34, 1.0), (35, 1.0), (36, 1.0), (37, 1.0), (38, 1.0), (39, 3.0), (40, 1.0), (41, 1.0), (42, 1.0), (43, 1.0), (44, 1.0), (45, 1.0), (46, 1.0), (47, 1.0), (48, 1.0), (49, 1.0), (50, 1.0), (51, 1.0), (52, 1.0), (53, 1.0), (54, 1.0), (55, 1.0)]\n",
      "[(25, 1.0), (56, 1.0), (57, 1.0), (58, 1.0), (59, 1.0), (60, 1.0), (61, 1.0), (62, 1.0), (63, 1.0), (64, 1.0), (65, 2.0), (66, 1.0), (67, 1.0), (68, 1.0), (69, 1.0), (70, 2.0), (71, 1.0), (72, 1.0), (73, 1.0), (74, 1.0), (75, 1.0), (76, 1.0), (77, 1.0), (78, 1.0), (79, 1.0), (80, 1.0), (81, 1.0), (82, 1.0), (83, 1.0)]\n",
      "[(35, 1.0), (84, 1.0), (85, 1.0), (86, 2.0), (87, 1.0), (88, 1.0), (89, 1.0), (90, 1.0), (91, 1.0), (92, 1.0), (93, 1.0), (94, 1.0), (95, 1.0), (96, 1.0), (97, 1.0), (98, 1.0), (99, 1.0), (100, 2.0), (101, 2.0), (102, 1.0), (103, 2.0), (104, 2.0), (105, 1.0), (106, 2.0), (107, 1.0), (108, 1.0), (109, 1.0), (110, 1.0), (111, 1.0), (112, 1.0), (113, 1.0), (114, 1.0), (115, 1.0), (116, 1.0), (117, 1.0), (118, 1.0), (119, 1.0), (120, 1.0)]\n",
      "[(87, 1.0), (101, 2.0), (121, 1.0), (122, 1.0), (123, 1.0), (124, 1.0), (125, 1.0), (126, 1.0), (127, 1.0), (128, 1.0), (129, 1.0), (130, 1.0), (131, 1.0), (132, 1.0), (133, 1.0), (134, 1.0), (135, 2.0), (136, 1.0), (137, 3.0), (138, 1.0), (139, 1.0), (140, 1.0), (141, 1.0), (142, 1.0), (143, 1.0), (144, 1.0), (145, 1.0), (146, 1.0)]\n",
      "[(114, 1.0), (146, 1.0), (147, 1.0), (148, 1.0), (149, 2.0), (150, 1.0), (151, 1.0), (152, 1.0), (153, 1.0), (154, 1.0), (155, 1.0), (156, 1.0), (157, 1.0), (158, 1.0), (159, 1.0), (160, 1.0), (161, 1.0)]\n",
      "[(88, 1.0), (91, 1.0), (147, 1.0), (149, 1.0), (153, 5.0), (162, 1.0), (163, 1.0), (164, 1.0), (165, 1.0), (166, 1.0), (167, 1.0), (168, 1.0), (169, 1.0), (170, 1.0), (171, 1.0), (172, 1.0), (173, 1.0), (174, 1.0), (175, 1.0), (176, 1.0), (177, 2.0), (178, 1.0), (179, 4.0), (180, 1.0), (181, 1.0), (182, 1.0)]\n",
      "[(21, 4.0), (34, 1.0), (37, 1.0), (48, 1.0), (69, 1.0), (153, 1.0), (179, 7.0), (183, 1.0), (184, 1.0), (185, 1.0), (186, 1.0), (187, 3.0), (188, 1.0), (189, 1.0), (190, 1.0), (191, 1.0), (192, 1.0), (193, 1.0), (194, 2.0), (195, 1.0), (196, 1.0), (197, 1.0), (198, 1.0), (199, 1.0), (200, 1.0), (201, 1.0), (202, 1.0)]\n",
      "[(21, 2.0), (35, 1.0), (149, 1.0), (153, 1.0), (175, 2.0), (201, 1.0), (202, 1.0), (203, 1.0), (204, 1.0), (205, 1.0), (206, 1.0), (207, 1.0), (208, 1.0), (209, 1.0), (210, 1.0), (211, 1.0), (212, 1.0), (213, 1.0), (214, 1.0)]\n",
      "[(21, 3.0), (48, 1.0), (133, 1.0), (153, 1.0), (175, 2.0), (177, 1.0), (193, 1.0), (201, 1.0), (202, 1.0), (205, 1.0), (209, 1.0), (212, 1.0), (213, 1.0), (215, 1.0), (216, 1.0), (217, 1.0), (218, 2.0), (219, 1.0), (220, 1.0), (221, 1.0), (222, 1.0), (223, 1.0), (224, 1.0), (225, 1.0), (226, 1.0), (227, 1.0), (228, 1.0), (229, 1.0)]\n",
      "[(21, 9.0), (48, 1.0), (69, 4.0), (104, 1.0), (133, 2.0), (149, 2.0), (175, 1.0), (177, 1.0), (193, 1.0), (201, 1.0), (202, 1.0), (205, 1.0), (211, 1.0), (218, 2.0), (220, 1.0), (225, 1.0), (229, 1.0), (230, 1.0), (231, 1.0), (232, 1.0), (233, 1.0), (234, 1.0), (235, 1.0), (236, 1.0), (237, 1.0), (238, 1.0), (239, 1.0), (240, 1.0), (241, 1.0), (242, 1.0), (243, 1.0), (244, 1.0), (245, 1.0), (246, 1.0)]\n",
      "[(5, 1.0), (34, 1.0), (42, 2.0), (72, 1.0), (117, 2.0), (126, 1.0), (228, 1.0), (238, 3.0), (244, 1.0), (247, 1.0), (248, 1.0), (249, 1.0), (250, 1.0), (251, 1.0), (252, 1.0), (253, 1.0), (254, 1.0), (255, 1.0), (256, 1.0), (257, 1.0), (258, 2.0), (259, 1.0), (260, 1.0), (261, 2.0), (262, 1.0), (263, 1.0), (264, 1.0), (265, 1.0), (266, 1.0), (267, 1.0), (268, 6.0), (269, 2.0), (270, 1.0), (271, 1.0), (272, 1.0), (273, 1.0), (274, 1.0), (275, 1.0), (276, 1.0), (277, 2.0), (278, 1.0), (279, 1.0), (280, 1.0), (281, 1.0), (282, 1.0), (283, 1.0), (284, 1.0), (285, 1.0), (286, 1.0), (287, 1.0), (288, 1.0), (289, 1.0), (290, 1.0), (291, 1.0), (292, 2.0), (293, 1.0), (294, 1.0), (295, 1.0), (296, 2.0), (297, 1.0), (298, 1.0), (299, 1.0), (300, 1.0), (301, 1.0), (302, 1.0), (303, 1.0), (304, 1.0), (305, 1.0), (306, 1.0), (307, 1.0), (308, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "#Cargo mi corpus\n",
    "corpus = corpora.MmCorpus('corpus.mm')\n",
    "print(corpus) #No lo carga en memoria\n",
    "#Para leer la representacion de bolda de palabras resultante de cada documento en el corpus\n",
    "for doc in corpus:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Vector Space Model - TF-IDF</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Suponga que tenemos el diccionario y corpus en disco\n",
    "#Primero necesitamos cargarlos\n",
    "dictionary = corpora.Dictionary.load('clase3.dict')\n",
    "corpus = corpora.MmCorpus('corpus.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construyendo un modelo VSM con ponderacion por TF-IDF\n",
    "tfidf = models.TfidfModel(corpus) #Inizializacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(135, 1), (288, 2)]\n",
      "[(135, 0.447213595499958), (288, 0.894427190999916)]\n"
     ]
    }
   ],
   "source": [
    "#Supongamos que queremos calcular la similitud de un nuevo documento con los documentos en el corpus\n",
    "#1. Construimos la representación vectorial del query\n",
    "query_doc = \"Amazon vs Microsoft\"\n",
    "query_doc_bow = dictionary.doc2bow(process(new_doc))\n",
    "print(query_doc_bow)\n",
    "print(tfidf[query_doc_bow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Ahora construimos la matrix t/d con los documentos contra los cuales queremos compara el query\n",
    "index = similarities.MatrixSimilarity(tfidf[corpus]) \n",
    "# Vamos a salvar el inidice resultante para no tener que recalcularlo cada vez que los necesitemos.\n",
    "index.save('clase3tfidf.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Una vez guardado lo podemos cargar sin necesidad de recalcularlo\n",
    "index = similarities.MatrixSimilarity.load('clase3tfidf.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.0), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.14518946), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0), (9, 0.0), (10, 0.0), (11, 0.07944595)]\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar las similitudes entre el documento query y el corpus ahora es muy facil!!!\n",
    "sims = index[tfidf[query_doc_bow]]\n",
    "print(list(enumerate(sims)))  # print (document_number, document_similarity) 2-tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
